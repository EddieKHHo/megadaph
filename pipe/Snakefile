"""Megadaph SNP and small indel variant calling pipeline."""
import os
from uuid import uuid4

from fmbiopy.fmlist import flatten
from fmbiopy.io import read_header
import numpy as np
from pandas import read_csv
from plumbum import (
    FG,
    local,
)
from plumbum.cmd import (
    blobtools,
    busco,
    calc_bed_length,
    calc_fasta_length,
    cat,
)
import scipy.stats as st

# ==============================================================================
# Set up
# ==============================================================================
configfile: "config.yml"

SAMPLE, PAIR = glob_wildcards(
    os.path.join(config['readsdir'], '{sample}.{pair}.fastq.gz'))

# Remove duplicate entries
SAMPLE = sorted(list(set(SAMPLE)))

# Genotype indices given by first two letters of sample name
GENOTYPE = list(set([x[0:2] for x in SAMPLE]))

GENOTYPE_TO_SAMPLE = {}
for genotype in GENOTYPE:
    GENOTYPE_TO_SAMPLE[genotype] = [x for x in SAMPLE if x.startswith(genotype)]

# Decontamination was done in several iterations, with each iteration consisting
# of:
# 1. Assembly
# 2. Blobplots/Covplots
# 3. Contamination filtering
# Outputs from contamination were reassembled. This was continued until minimal
# contamination was evident in blobplots

# Assembly/Decontam iterations which should be plotted
PLOT_ITER = ['1', '2', '_final']
# Assembly iterations which should be targeted for decontamination
DECONTAM_ITER = ['1', '2', '_reduced']
# Iterations of De Novo assembly with spades
ASSEMBLY_ITER = ['1', '2', '3']

VARIANT_TYPES = ['snps', 'indels', 'other']

# Suffixes for various types of fasta indices
FASTA_INDICES = {
    'bowtie2': ['.1.bt2', '.2.bt2', '.3.bt2', '.4.bt2', '.rev.1.bt2',
                '.rev.2.bt2'],
    'picard': ['.dict'],
    'samtools': ['.fasta.fai']
}
INDEX_SUFFIXES = flatten(FASTA_INDICES.values())

# ==============================================================================
# Helper functions
# ==============================================================================

def match_reads(wildcards):
    """Match reads to the current wildcards."""
    try:
        smp = wildcards.sample
    except AttributeError:
        smp = wildcards.genotype + 'SC'
    outdict = {}
    read_dir = 'output'
    if wildcards.iter in ['_final', '_ref']:
        read_dir = os.path.join(read_dir, 'filter_contamination_reduced')
    elif wildcards.iter == '_reduced':
        read_dir = os.path.join(read_dir, 'filter_contamination2')
    elif int(wildcards.iter) == 1:
        read_dir = os.path.join(read_dir, 'quality_trim')
    else:
        prev_iter = str(int(wildcards.iter) - 1)
        read_dir = os.path.join(read_dir, 'filter_contamination' + prev_iter)
    outdict['fwd_reads'] = os.path.join(read_dir, smp + '.R1.fastq.gz')
    outdict['rev_reads'] = os.path.join(read_dir, smp + '.R2.fastq.gz')
    outdict['unpaired_reads'] = os.path.join(
            read_dir, smp + '.unpaired.fastq.gz')
    return outdict


def get_sample(wildcards):
    """Get the sample from the wildcards.

    Return the starting control if wildcards.sample doesn't exist.
    """
    try:
        return wildcards.sample
    except AttributeError:
        return wildcards.genotype + 'SC'


def match_filter_alignment(wildcards):
    """Get an alignment which is targeted for contamination filtering."""
    smp = get_sample(wildcards)
    outdict = dict()
    if wildcards.iter in ['_reduced', '_final']:
        dir = os.path.join('output', 'align_to_assembly_reduced')
    else:
        dir = os.path.join('output', 'align_to_assembly' + wildcards.iter)
    outdict['bam'] = os.path.join(dir, smp + '.bam')
    outdict['bai'] = os.path.join(dir, smp + '.bam.bai')
    return outdict


def get_genotype(wildcards):
    """Get the genotype from the wildcards."""
    try:
        return wildcards.genotype
    except AttributeError:
        return ''.join(wildcards.sample[0:2])


def get_starting_control(wildcards):
    """Get the starting control from the wildcards."""
    return get_genotype(wildcards) + 'SC'


def match_assembly(wildcards):
    """Match the current assembly and indices to the wildcards."""
    outdict = dict()
    if hasattr(wildcards, 'iter'):
        if wildcards.iter == '_ref':
            outdict['assembly'] = config['nuclear_reference']
            prefix = os.path.splitext(outdict['assembly'])[0]
            dir = os.path.dirname(outdict['assembly'])
            outdict['indices'] = [
                os.path.join(dir, (prefix + suffix))
                for suffix in INDEX_SUFFIXES]
            return outdict

    starting_control = get_starting_control(wildcards)
    if wildcards.iter == '_final':
        dir = os.path.join('output', 'filter_zero_coverage')
    else:
        dir = os.path.join('output', 'length_filter_assembly' + wildcards.iter)
    outdict['assembly'] = os.path.join(dir, (starting_control + '.fasta'))
    outdict['indices'] = [os.path.join(dir, starting_control + suffix)
            for suffix in INDEX_SUFFIXES]
    return outdict


def match_final_assembly(wildcards):
    """Get the final assembly from the wildcards."""
    starting_control = get_starting_control(wildcards)
    outdict = {}
    prefix = os.path.join('output', 'filter_zero_coverage', starting_control)
    outdict['assembly'] = prefix + '.fasta'
    outdict['indices'] = [prefix + suffix for suffix in INDEX_SUFFIXES]
    return outdict


def build_filter_expression(parameters):
    """Given a set of filter conditions, build a GATK filter expression."""
    param_list = []
    for key, value in parameters.items():
        if not isinstance(parameters[key], str):
            for item in parameters[key]:
                param_list.append(' '.join([key, item]))
        else:
            param_list.append(' '.join([key, value]))
    filter_expression = '"' + ' || '.join(param_list) + '"'
    return filter_expression


def clear_directory(dir):
    """Delete the contents of a directory."""
    dir = local.path(dir)
    dir.delete()
    dir.mkdir()

# ==============================================================================
# Pipeline Outputs
# ==============================================================================
rule all:
    input:
        'output/multiqc/multiqc.html',
        expand("output/produce_blobplots{iter}/{genotype}/family/{genotype}."
               "bestsum.family.p20.span.100.blobplot.cov0.png",
               genotype=GENOTYPE, iter=PLOT_ITER),
        expand("output/produce_covplots{iter}/{sample}.covsum.png",
               sample=SAMPLE, iter=PLOT_ITER),
        'output/count_callable_loci/line_info.csv',
        expand('output/plot_variant_quality_metrics/{sample}.svg',
               sample=SAMPLE),
        expand('output/filter_shared_variants/{sample}.tsv',
               sample=SAMPLE),
        expand('output/genotype_gvcfs/{sample}.vcf', sample=SAMPLE),
        expand('output/remove_filtered_variants/{sample}.{vartype}.vcf',
                sample=SAMPLE, vartype=VARIANT_TYPES)

# ==============================================================================
# Utility Rules
# ==============================================================================

rule index_bam:
    input:
        'output/{dir}/{group}.bam'
    output:
        'output/{dir}/{group}.bam.bai'
    shell:
        'samtools index {input}'

rule index_fasta:
    input:
        fasta = 'output/{dir}/{group}.fasta'
    output:
        ['output/{dir}/{group}' + suffix for suffix in INDEX_SUFFIXES]
    run:
        index_prefix = os.path.splitext(input.fasta)[0]
        picard_dict = index_prefix + '.dict'
        shell(' '.join(['bowtie2-build', input.fasta, index_prefix]))
        shell(' '.join(['samtools', 'faidx', input.fasta]))
        shell('picard CreateSequenceDictionary R=' + input.fasta +
              ' O=' + picard_dict)

rule align_to_assembly:
    input:
        unpack(match_reads),
        unpack(match_assembly)
    output:
        bam = 'output/align_to_assembly{iter}/{sample}.bam'
    threads: 12
    resources:
        mem_mb = 20000
    params:
        max_insert = 1000
    log:
        "output/align_to_assembly{iter}/log/{sample}.log"
    run:
        index_prefix = os.path.splitext(input.assembly)[0]
        shell('bowtie2 -1 {input.fwd_reads} -2 {input.rev_reads} '
              '-U {input.unpaired_reads} -X {params.max_insert} '
              '-x ' + index_prefix + ' --rg-id={wildcards.sample} '
              '--rg=LIB:{wildcards.sample} --rg=PL:illumina '
              '--rg=SM:{wildcards.sample} -p {threads} | '
              'samtools view -bh - | '
              'samtools sort - > {output.bam} 2> {log}')

# ==============================================================================
# Build extra input files
# ==============================================================================

rule build_diamond_database:
    output:
        "output/build_diamond_database/uniprot_ref_proteomes.fasta",
        "output/build_diamond_database/uniprot_ref_proteomes.dmnd",
        "output/build_diamond_database/uniprot_ref_proteomes.taxids"
    threads: 16
    script:
        "scripts/build_diamond_database.py"

# ==============================================================================
# Quality Control / Trimming
# ==============================================================================

rule trim_adapters:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        fwd_reads = temp('output/trim_adapters/{sample}.R1.fastq.gz'),
        rev_reads = temp('output/trim_adapters/{sample}.R2.fastq.gz')
    log: "output/trim_adapters/log/{sample}.log"
    threads: 10
    params:
        ref = config['adapters'],
        k = '23',
        ktrim = 'r',
        mink = '4',
        hdist = '1'
    shell:
        'bbduk.sh '
        'in={input.fwd_reads} in2={input.rev_reads} '
        'threads={threads} '
        'out={output.fwd_reads} out2={output.rev_reads} '
        'ref={params.ref} '
        'k={params.k} '
        'ktrim={params.ktrim} '
        'mink={params.mink} '
        'hdist={params.hdist} '
        'tpe tbo 2> {log}'


rule merge_reads:
    input:
        fwd_reads = 'output/trim_adapters/{sample}.R1.fastq.gz',
        rev_reads = 'output/trim_adapters/{sample}.R2.fastq.gz'
    output:
        fwd_reads = 'output/merge_reads/{sample}.R1.fastq.gz',
        rev_reads = 'output/merge_reads/{sample}.R2.fastq.gz',
        merged_reads = 'output/merge_reads/{sample}.unpaired.fastq.gz',
        insert_hist = 'output/merge_reads/{sample}.hist'
    log:
        'output/merge_reads/log/{sample}.log'
    threads: 12
    params:
        vstrict = 't'
    shell:
        'bbmerge.sh '
        'in1={input.fwd_reads} in2={input.rev_reads} '
        'threads={threads} '
        'out={output.merged_reads} '
        'outu={output.fwd_reads} outu2={output.rev_reads} '
        'ihist={output.insert_hist} '
        'vstrict={params.vstrict} 2> {log}'


rule quality_trim:
    input:
        fwd_reads = 'output/merge_reads/{sample}.R1.fastq.gz',
        rev_reads = 'output/merge_reads/{sample}.R2.fastq.gz',
        merged_reads = 'output/merge_reads/{sample}.unpaired.fastq.gz',
    output:
        fwd_reads = 'output/quality_trim/{sample}.R1.fastq.gz',
        rev_reads = 'output/quality_trim/{sample}.R2.fastq.gz',
        merged_reads = 'output/quality_trim/{sample}.unpaired.fastq.gz'
    log:
        paired = 'output/quality_trim/log/{sample}.paired.log',
        unpaired = 'output/quality_trim/log/{sample}.unpaired.log'
    threads: 10
    params:
        qtrim = 'rl',
        trimq = '20',
        minlen = '50'
    run:
        shell('bbduk.sh '
              'in={input.fwd_reads} in2={input.rev_reads} '
              'out={output.fwd_reads} out2={output.rev_reads} '
              'threads={threads} '
              'qtrim={params.qtrim} '
              'trimq={params.trimq} '
              'minlen={params.minlen} 2> {log.paired}')
        shell('bbduk.sh '
              'in={input.merged_reads} '
              'out={output.merged_reads} '
              'qtrim={params.qtrim} '
              'trimq={params.trimq} '
              'minlen={params.minlen} '
              'threads={threads} 2> {log.unpaired}')

# ==============================================================================
# Assembly-Decontamination Iteration
# ==============================================================================

rule assembly:
    input:
        unpack(match_reads)
    output:
        assembly = 'output/assembly{iter}/{genotype}SC.fasta'
    threads: 8
    resources:
        mem_mb = 150000
    run:
        outdir = local.path('output') / ('assembly' + wildcards.iter)
        clear_directory(outdir)
        shell(
            'python2 util/spades/bin/spades.py '
            '-1 {input.fwd_reads} -2 {input.rev_reads} '
            '-s {input.unpaired_reads} '
            '--threads {threads} '
            '-o ' + str(outdir))
        (outdir / 'scaffolds.fasta').move(output.assembly)

rule length_filter_assembly:
    input:
        lambda wildcards: (
            os.path.join('output', 'filter_mitochondrial_scaffolds',
                         wildcards.genotype + 'SC.fasta')
            if wildcards.iter == '_reduced' else
            os.path.join('output', 'assembly' + wildcards.iter,
                         wildcards.genotype + 'SC.fasta'))
    output:
        'output/length_filter_assembly{iter}/{genotype}SC.fasta'
    run:
        if wildcards.iter == '_reduced':
            minlen = '1000'
        else:
            minlen = '200'
        shell('length_filter_fasta.pl ' + minlen + ' {input} > {output}')

rule blast_assembly:
    input:
        unpack(match_assembly)
    output:
        'output/blast_assembly{iter}/{genotype}SC.tsv'
    threads: 10
    params:
        max_target_seqs = '5',
        evalue = '1e-25',
        max_hsps = '1',
    shell:
        'blastn '
        '-db {config[blast_database]} '
        '-task megablast '
        '-max_target_seqs {params.max_target_seqs} '
        '-max_hsps {params.max_hsps} '
        '-evalue {params.evalue} '
        '-query {input.assembly} '
        '-out {output} '
        '-outfmt \'6 qseqid staxids bitscore std\' '
        '-num_threads {threads}'


rule diamond_blast_assembly:
    input:
        unpack(match_assembly),
        db = 'output/build_diamond_database/uniprot_ref_proteomes.dmnd'
    output:
        'output/diamond_blast_assembly{iter}/{genotype}SC.out'
    params:
        max_target_seqs = '1',
        evalue = '1e-25'
    threads: 16
    resources:
        mem_mb = 50000
    shell:
        'diamond blastx '
        '--query {input.assembly} '
        '--max-target-seqs {params.max_target_seqs} '
        '--threads {threads} '
        '--sensitive '
        '--db {input.db} '
        '--evalue {params.evalue} '
        '--outfmt 6 '
        '--out {output}'

# Add taxonomic information to diamond blast results
rule taxify_diamond_blasts:
    input:
        diamond_hits = 'output/diamond_blast_assembly{iter}/{genotype}SC.out',
        taxids = 'output/build_diamond_database/uniprot_ref_proteomes.taxids'
    output:
        'output/diamond_blast_assembly{iter}/{genotype}SC.taxified.out',
    params:
        output_prefix = 'output/diamond_blast_assembly{iter}/'
    shell:
        'blobtools taxify -f {input.diamond_hits} -m {input.taxids} -s 0 -t 2 '
        '-o {params.output_prefix}'

# Get coverage at each base in the alignment
rule convert_alignment_to_cov:
    input:
        unpack(match_assembly),
        bam = 'output/align_to_assembly{iter}/{sample}.bam',
        bai = 'output/align_to_assembly{iter}/{sample}.bam.bai',
    output:
        'output/convert_alignment_to_cov{iter}/{sample}.bam.cov'
    resources:
        mem_mb = 20000
    params:
        output_prefix = 'output/convert_alignment_to_cov{iter}/'
    log:
        'output/convert_alignment_to_cov{iter}/logs/{sample}.log'
    shell:
        'blobtools map2cov '
        '-i {input.assembly} '
        '-b {input.bam} '
        '-o {params.output_prefix} 2> {log}'

rule init_blobtools_database:
    input:
        unpack(match_assembly),
        blast_hits = 'output/blast_assembly{iter}/{genotype}SC.tsv',
        covs = lambda wildcards: [
            '/'.join([
                'output/convert_alignment_to_cov' + wildcards.iter,
                smp + '.bam.cov'])
            for smp in GENOTYPE_TO_SAMPLE[wildcards.genotype]],
        diamond_hits = (
            'output/diamond_blast_assembly{iter}/{genotype}SC.taxified.out')
    output:
        'output/init_blobtools_database{iter}/{genotype}.blobDB.json'
    resources:
        mem_mb = 20000
    params:
        output_prefix = 'output/init_blobtools_database{iter}/{genotype}'
    log:
        'output/init_blobtools_database{iter}/log/{genotype}.log'
    run:
        cov_args = '-c ' + ' -c '.join(input.covs)
        shell('blobtools create '
              '-i {input.assembly} '
              '--title {wildcards.genotype} '
              '--out {params.output_prefix} '
              '--hitsfile {input.blast_hits} --hitsfile {input.diamond_hits} '
              '--nodes {config[nodes_dmp]} --names {config[names_dmp]} ' +
              cov_args + ' > {log} 2>&1')

rule produce_blobplots:
    input:
        db = 'output/init_blobtools_database{iter}/{genotype}.blobDB.json'
    output:
        ('output/produce_blobplots{iter}/{genotype}/family/{genotype}.bestsum.'
         'family.p20.span.100.blobplot.cov0.png'),
        ('output/produce_blobplots{iter}/{genotype}/genus/{genotype}.bestsum.'
         'genus.p20.span.100.blobplot.cov0.png'),
        ('output/produce_blobplots{iter}/{genotype}/order/{genotype}.bestsum.'
         'order.p20.span.100.blobplot.cov0.png'),
        ('output/produce_blobplots{iter}/{genotype}/phylum/{genotype}.bestsum.'
         'phylum.p20.span.100.blobplot.cov0.png'),
        ('output/produce_blobplots{iter}/{genotype}/species/{genotype}.bestsum.'
         'species.p20.span.100.blobplot.cov0.png')
    threads: 5
    resources:
        mem_mb = 40000
    params:
        outdir = 'output/produce_blobplots{iter}/{genotype}'
    run:
        processes = []
        for rank in ['family', 'genus', 'order', 'phylum', 'species']:
            outdir = params.outdir + '/' + rank + '/'
            processes.append(
                blobtools.popen([
                    'blobplot', '-p', '20', '-r', rank, '-i', input.db, '-o',
                    outdir]))
        for process in processes:
            process.communicate()

rule produce_covplots:
    input:
        cov = 'output/convert_alignment_to_cov{iter}/{sample}.bam.cov',
        db = lambda wildcards: '/'.join([
            'output/init_blobtools_database' + wildcards.iter,
            get_genotype(wildcards) + '.blobDB.json'])
    output:
        'output/produce_covplots{iter}/{sample}.covsum.png'
    params:
        output_prefix = 'output/produce_covplots{iter}/{sample}',
        rank = 'superkingdom'
    run:
        shell('blobtools covplot -p 20 -c {input.cov} '
              '-i {input.db} -r {params.rank} '
              '-o {params.output_prefix}')

        outdir = local.path(params.output_prefix).dirname
        output_files = outdir.glob(wildcards.sample + '*')
        for f in output_files:
            extensions = '.'.join(f.name.split('.')[-2:])
            newname = outdir / (wildcards.sample + '.' + extensions)
            f.move(newname)

rule produce_blobtable:
    input:
        db = 'output/init_blobtools_database{iter}/{genotype}.blobDB.json'
    output:
        'output/produce_blobtable{iter}/{genotype}.blobDB.bestsum.table.txt'
    params:
        outdir = 'output/produce_blobtable{iter}/'
    shell:
        'blobtools view -r all -b -i {input.db} -o {params.outdir}'

rule find_contam_contigs:
    input:
        'output/produce_blobtable{iter}/{genotype}.blobDB.bestsum.table.txt'
    output:
        'output/find_contam_contigs{iter}/{genotype}SC.txt'
    shell:
        'scripts/find_contam_contigs{wildcards.iter}.R {input} > {output}'


# List the contigs which passed the filtering step
rule generate_inclusion_list:
    input:
        unpack(match_assembly),
        exclusions = lambda wildcards: (
            os.path.join('output', 'combine_exclusions',
                         wildcards.genotype + 'SC.txt')
            if wildcards.iter == '_reduced'
            else os.path.join('output', 'find_contam_contigs' + wildcards.iter,
                              wildcards.genotype + 'SC.txt'))
    output:
        'output/generate_inclusion_list{iter}/{genotype}SC.txt'
    shell:
        'list_csomes {input.assembly} | '
        'complement.sh /dev/stdin {input.exclusions} > {output}'


rule filter_contamination:
    input:
        unpack(match_filter_alignment),
        passing_contigs = lambda wildcards: os.path.join(
            'output/generate_inclusion_list' + wildcards.iter,
            get_genotype(wildcards) + 'SC.txt'),
    output:
        fwd = 'output/filter_contamination{iter}/{sample}.R1.fastq.gz',
        rev = 'output/filter_contamination{iter}/{sample}.R2.fastq.gz',
        unpaired = (
            'output/filter_contamination{iter}/{sample}.unpaired.fastq.gz')
    threads: 16
    resources:
        mem_mb = 60000
    params:
        output_prefix = 'output/filter_contamination{iter}/{sample}'
    log:
        'output/filter_contamination{iter}/logs/{sample}.log'
    run:
        # This step removes all reads which are not mapped to a contig in the
        # inclusion list (this includes unmapped reads).
        shell('extract_csomes.py '
              '-n 200 '
              '-p {threads} '
              '-o {params.output_prefix} '
              '-i {input.passing_contigs} '
              '-f fastq {input.bam} 2> {log}')

        outdir = local.path(output.unpaired).dirname
        tmp = outdir / (uuid4().hex + '.fastq.gz')
        # Remove all unpaired reads which are not merged
        shell("zcat {output.unpaired} | "
              "paste - - - - | "
              "awk 'length($2) >= 151' | sed 's/\\t/\\n/g' | "
              "gzip - > " + str(tmp))
        tmp.move(output.unpaired)

# ==============================================================================
# Reduction/Scaffolding
# ==============================================================================

rule redundans:
    input:
        fwd_reads = 'output/filter_contamination2/{genotype}SC.R1.fastq.gz',
        rev_reads = 'output/filter_contamination2/{genotype}SC.R2.fastq.gz',
        unpaired_reads = (
            'output/filter_contamination2/{genotype}SC.unpaired.fastq.gz'),
        assembly = 'output/assembly3/{genotype}SC.fasta'
    output:
        fasta = 'output/redundans/{genotype}SC.fasta'
    params:
        outdir = 'output/redundans/{genotype}'
    threads: 16
    resources:
        mem_mb = 100000
    log:
        'output/redundans/log/{genotype}.log'
    run:
        clear_directory(params.outdir)
        shell('rm -rf {params.outdir}')
        shell('python2 {config[redundans_bin]} -v -f {input.assembly} '
              '-i {input.fwd_reads} {input.rev_reads} {input.unpaired_reads} '
              '-o {params.outdir} '
              '-r {config[nuclear_reference]} '
              '-t {threads} '
              '--usebwa 2> {log}')
        (local.path(params.outdir) / 'scaffolds.reduced.fa').move(output.fasta)

rule blast_against_mitochondria:
    input:
        'output/redundans/{genotype}SC.fasta'
    output:
        list = 'output/blast_against_mitochondria/{genotype}SC.txt',
        blast = 'output/blast_against_mitochondria/{genotype}SC.tsv'
    run:
        # BLAST assembly vs. mtdna
        shell('blastn -subject {input} '
              '-query {config[mitochondrial_reference]} '
              '-num_alignments 10 -outfmt 6 > {output.blast}')
        # Get the contig names
        shell('cat {output.blast} | cut -f 2 | uniq -d > {output.list}')

rule filter_mitochondrial_scaffolds:
    input:
        blast = 'output/blast_against_mitochondria/{genotype}SC.tsv',
        assembly = 'output/redundans/{genotype}SC.fasta'
    output:
        fasta = 'output/filter_mitochondrial_scaffolds/{genotype}SC.fasta',
        txt = 'output/filter_mitochondrial_scaffolds/{genotype}SC.txt'
    shell:
        'scripts/filter_mitochondrial_scaffolds.py '
        '--blast {input.blast} '
        '--assembly {input.assembly} '
        '--outfasta {output.fasta} '
        '--outtxt {output.txt}'

rule filter_zero_coverage:
    input:
        bam = 'output/align_to_assembly_reduced/{genotype}SC.bam',
        bai = 'output/align_to_assembly_reduced/{genotype}SC.bam.bai',
        fasta = 'output/length_filter_assembly_reduced/{genotype}SC.fasta'
    output:
        cov = 'output/filter_zero_coverage/{genotype}SC.bam.cov',
        fasta = 'output/filter_zero_coverage/{genotype}SC.fasta',
        inclusions = 'output/filter_zero_coverage/{genotype}SC.txt'
    run:
        cov_output_prefix = 'output/filter_zero_coverage/'
        cov = os.path.join(cov_output_prefix,
                           os.path.basename(input.bam) + '.cov')
        shell('blobtools map2cov '
              '-i {input.fasta} '
              '-b {input.bam} '
              '-o ' + cov_output_prefix)
        shell('scripts/get_high_depth_contigs.awk ' + cov + ' > '
              '{output.inclusions}')
        shell('seqtk subseq {input.fasta} {output.inclusions} > {output.fasta}')

rule combine_exclusions:
    input:
        cov = 'output/filter_zero_coverage/{genotype}SC.bam.cov',
        mtexclude = 'output/filter_mitochondrial_scaffolds/{genotype}SC.txt'
    output:
        'output/combine_exclusions/{genotype}SC.txt'
    shell:
        'cat <(scripts/get_low_depth_contigs.awk {input.cov}) '
        '{input.mtexclude} > {output}'

rule repeatmasker:
    input:
        'output/filter_zero_coverage/{genotype}SC.fasta'
    output:
        masked = 'output/repeatmasker/{genotype}SC.fasta',
        out = 'output/repeatmasker/{genotype}SC.out'
    threads: 8
    run:
        shell('RepeatMasker '
              '-lib {config[repeat_library]} '
              '-s '
              '-par {threads} '
              '{input}')
        input_dir = local.path(input).dirname
        output_dir = local.path(output.masked).dirname
        tmp_outputs = [input_dir / (wildcards.genotype + 'SC' + suffix)
                       for suffix in ['.fasta.masked', '.fasta.out']]
        for tmp, final in zip(tmp_outputs, output):
            tmp.move(final)

rule repeatmasker2bed:
    input:
        'output/repeatmasker/{genotype}SC.out'
    output:
        'output/repeatmasker/{genotype}SC.bed'
    shell:
        'rmsk2bed < {input} | cut -f 1-3 > {output}'

# ==============================================================================
# Variant Calling Pre-Processing
# ==============================================================================

rule mark_duplicates:
    input:
        bam = 'output/align_to_assembly_final/{sample}.bam',
        bai = 'output/align_to_assembly_final/{sample}.bam.bai'
    output:
        bam = 'output/mark_duplicates/{sample}.bam',
        logfile = 'output/mark_duplicates/log/{sample}.log'
    resources:
        mem_mb = 30000
    log:
        'output/mark_duplicates/log/{sample}.log'
    shell:
        'picard MarkDuplicates '
        'I={input.bam} '
        'O={output.bam} '
        'REMOVE_DUPLICATES=True '
        'M={log}'

# ==============================================================================
# Variant Calling
# ==============================================================================

rule call_variants:
    input:
        unpack(match_final_assembly),
        bam = 'output/mark_duplicates/{sample}.bam',
        bai = 'output/mark_duplicates/{sample}.bam.bai',
        repeats = lambda wildcards: (
                os.path.join('output', 'repeatmasker',
                             get_genotype(wildcards) + 'SC.bed'))
    output:
        gvcf = 'output/call_variants/{sample}.g.vcf'
    resources:
        mem_mb = 20000
    params:
        snp_heterozygosity = 0.015,
        indel_heterozygosity = 0.01
    log:
        'output/call_variants/log/{sample}.log'
    shell:
        'java -jar {config[gatk3_jar]} -T HaplotypeCaller '
        '-R {input.assembly} '
        '-I {input.bam} '
        '--excludeIntervals {input.repeats} '
        '--emitRefConfidence GVCF '
        '-o {output} '
        '--heterozygosity {params.snp_heterozygosity} '
        '--output_mode EMIT_ALL_SITES '
        '--indel_heterozygosity {params.indel_heterozygosity} '

rule genotype_gvcfs:
    input:
        unpack(match_final_assembly),
        gvcf = 'output/call_variants/{sample}.g.vcf',
    output:
        'output/genotype_gvcfs/{sample}.vcf'
    params:
        snp_heterozygosity = 0.015,
        indel_heterozygosity = 0.01
    shell:
        'java -jar {config[gatk3_jar]} -T GenotypeGVCFs '
        '-R {input.assembly} '
        '-V {input.gvcf} '
        '-o {output} '
        '--includeNonVariantSites '
        '--heterozygosity {params.snp_heterozygosity} '
        '--indel_heterozygosity {params.indel_heterozygosity} '

rule combine_gvcfs:
    input:
        unpack(match_final_assembly),
        gvcf = lambda wildcards: [os.path.join(
            'output', 'call_variants', sample + '.g.vcf')
            for sample in GENOTYPE_TO_SAMPLE[wildcards.genotype]],
        repeats = 'output/repeatmasker/{genotype}SC.bed'
    output:
        'output/combine_gvcfs/{genotype}.g.vcf'
    resources:
        mem_mb = 20000
    run:
        vcf_param = '-V ' + ' -V '.join(input.gvcf) + ' '
        shell('java -jar {config[gatk3_jar]} -T CombineGVCFs ' +
              vcf_param +
              '-R {input.assembly} '
              '-o {output} '
              '--excludeIntervals {input.repeats}')

rule sort_combined_gvcfs:
    input:
        'output/combine_gvcfs/{genotype}.g.vcf'
    output:
        'output/sort_combined_gvcfs/{genotype}.g.vcf'
    resources:
        mem_mb = 20000
    shell:
        'picard SortVcf I={input} O={output}'

# Create multisample genotyped vcf files
rule joint_genotype_gvcfs:
    input:
        unpack(match_final_assembly),
        gvcf = 'output/sort_combined_gvcfs/{genotype}.g.vcf'
    output:
        'output/joint_genotype_gvcfs/{genotype}.vcf'
    params:
        snp_heterozygosity = 0.015,
        indel_heterozygosity = 0.01
    resources:
        mem_mb = 30000
    shell:
        'java -jar {config[gatk3_jar]} -T GenotypeGVCFs '
        '-R {input.assembly} '
        '-V {input.gvcf} '
        '-o {output} '
        '--includeNonVariantSites '
        '--heterozygosity {params.snp_heterozygosity} '
        '--indel_heterozygosity {params.indel_heterozygosity}'

# Remove variants which have coverage outside of the target range.
rule depth_filter_variants:
    input:
        'output/joint_genotype_gvcfs/{genotype}.tsv'
    output:
        tsv = 'output/depth_filter_variants/{genotype}.tsv',
        summary = 'output/depth_filter_variants/{genotype}.txt'
    params:
        min = 20,
        max = 100
    shell:
        'scripts/depth_filter_variants.py '
        '--min-depth {params.min} '
        '--max-depth {params.max} '
        '--summary {output.summary} '
        '--output {output.tsv} {input}'


# Count the total number of callable sites for each sample and add the
# information to the line info metadata.
rule count_callable_loci:
    input:
        assembly = expand('output/filter_zero_coverage/{genotype}SC.fasta',
                genotype=GENOTYPE),
        depth_filtered = expand('output/depth_filter_variants/{genotype}.txt',
                genotype=GENOTYPE),
        masked = expand('output/repeatmasker/{genotype}SC.bed',
                genotype=GENOTYPE)
    output:
        'output/count_callable_loci/line_info.csv'
    run:
        line_info = read_csv(config['line_info'])
        line_info['bases_surveyed'] = np.nan
        input_sources = zip(tuple(input + [GENOTYPE]))
        for assembly, masked, depth_filtered, genotype in input_sources:
            num_masked = int(calc_bed_length(masked).rstrip())
            with open(depth_filtered, 'r') as inp:
                num_depth_filtered = int(inp.read().rstrip())
            assembly_length = int(calc_fasta_length(assembly).rstrip())
            num_callable = assembly_length - num_depth_filtered - num_masked
            genotype_idx = [x.startswith(genotype) for x in line_info['sample']]
            line_info.loc[genotype_idx, 'bases_surveyed'] = num_callable


# Filter out variants which are present in multiple samples
rule filter_shared_variants:
    input:
        expand('output/depth_filter_variants/{genotype}.tsv',
                genotype=GENOTYPE)
    output:
        expand('output/filter_shared_variants/{sample}.tsv',
                sample=SAMPLE)
    params:
        cutoff = 0.2
    resources:
        mem_mb = 20000
    run:
        outdir = os.path.dirname(output[0])
        for filename in input:
            shell('scripts/filter_shared_alleles.py '
                  '--het-cutoff {params.cutoff} '
                  '--outdir ' + outdir + ' ' +
                  filename)

# Filter variants in the per-sample .vcf files using the the merged, filtered
# variant tables.
rule filter_per_sample_variants:
    input:
        tsv = 'output/filter_shared_variants/{sample}.tsv',
        vcf = 'output/genotype_gvcfs/{sample}.vcf'
    output:
        'output/filter_per_sample_variants/{sample}.vcf'
    shell:
        # Extract the vcf header
        "grep '^#' {input.vcf} > {output};"

        # Filter out all variant positions in input.vcf which are not in
        # input.tsv
        "cat {input.tsv} | "
        "awk '{{printf \"%s\\t%s\\t\\n\", $1, $2}}' | "
        "grep -f /dev/stdin {input.vcf} >> {output}"

rule extract_variant_coverage:
    input:
        'output/filter_per_sample_variants/{sample}.vcf'
    output:
        'output/extract_variant_coverage{iter}/{sample}.txt',
    shell:
        'extract_variant_coverage.py -o {output} {input}'

rule select_variants:
    input:
        unpack(match_final_assembly),
        vcf = 'output/filter_per_sample_variants/{sample}.vcf',
    output:
        'output/select_variants/{sample}.{vartype}.vcf'
    run:
        if wildcards.vartype == 'snps':
            criteria = '--select-type-to-include SNP'
        elif wildcards.vartype == 'indels':
            criteria = '--select-type-to-include INDEL'
        elif wildcards.vartype == 'other':
            criteria = (
                '--select-type-to-exclude SNP '
                '--select-type-to-exclude INDEL')
        shell('gatk SelectVariants '
              '-R {input.assembly} '
              '-V {input.vcf} '
              '-O {output} ' +
              criteria)

rule exclude_nonvariants:
    input:
        unpack(match_final_assembly),
        vcf = 'output/select_variants/{sample}.{vartype}.vcf',
    output:
        vcf = 'output/exclude_nonvariants/{sample}.{vartype}.vcf'
    shell:
        'gatk SelectVariants '
        '-R {input.assembly} '
        '-V {input.vcf} '
        '--exclude-non-variants '
        '-O {output.vcf}'

rule extract_variant_quality_metrics:
    input:
        unpack(match_final_assembly),
        vcf = 'output/select_variants/{sample}.{vartype}.vcf'
    output:
        'output/extract_variant_quality_metrics/{sample}.{vartype}.q.tsv'
    shell:
        'gatk VariantsToTable '
        '-R {input.assembly} '
        '-V {input.vcf} '
        '-F CHROM -F QUAL -F QD -F DP -F MQ -F MQRankSum -F FS -GF AD '
        '-F ReadPosRankSum -F SOR '
        '-O {output}'

rule plot_variant_quality_metrics:
    input:
        snps = (
            'output/extract_variant_quality_metrics/{sample}.snps.q.tsv'),
        indels = (
            'output/extract_variant_quality_metrics/{sample}.indels.q.tsv')
    output:
        'output/plot_variant_quality_metrics/{sample}.svg'
    script:
        'scripts/plot_variant_quality_scores.R'

# Filter variants using GATK quality metrics
rule filter_variants:
    input:
        unpack(match_final_assembly),
        vcf = 'output/select_variants/{sample}.{vartype}.vcf',
    output:
        'output/filter_variants/{sample}.{vartype}.vcf'
    params:
    run:
        if wildcards.vartype == 'snps':
            filters = {
                'QD': '< 2.0',
                'FS': '> 60.0',
                'MQ': '< 35.0',
                'SOR': '> 4.0',
                'MQRankSum': '< -12.5',
                'ReadPosRankSum': ['< -10.0', '> 10.0']
            }
        elif wildcards.vartype in ['indels', 'other']:
            filters = {
                'QD': '< 5.0',
                'FS': '> 200.0',
                'QUAL': '< 100.0',
                'SOR': '> 10.0',
                'ReadPosRankSum': ['< -10.0', '> 10.0']
            }
        filter_expression = build_filter_expression(filters)
        shell('gatk VariantFiltration '
              '-R {input.assembly} '
              '-V {input.vcf} '
              '-O {output} '
              '--filter-name {wildcards.vartype}_filter '
              '--filter-expression ' + filter_expression)

rule calc_number_filtered_sites:
    input:
        pre = 'output/select_variants/{sample}.{vartype}.vcf',
        post = 'output/filter_variants/{sample}.{vartype}.vcf'
    output:
        'output/calc_number_filtered_sites/{sample}.{vartype}.txt'
    shell:
        'echo "Sites prefilter: " >> {output}; '
        'grep -vc "^#" {input.pre} >>  {output}; '
        'echo "\nSites postfilter: " >> {output}; '
        'grep -vc "^#" {input.post} >> {output}'

rule remove_filtered_variants:
    input:
        unpack(match_final_assembly),
        vcf = 'output/filter_variants/{sample}.{vartype}.vcf',
    output:
        'output/remove_filtered_variants/{sample}.{vartype}.vcf'
    shell:
        'gatk SelectVariants '
        '-R {input.assembly} '
        '-V {input.vcf} '
        '-O {output} '
        '--exclude-filtered '
        '--exclude-non-variants'

rule vcf_to_tsv:
    input:
        unpack(match_final_assembly),
        vcf = 'output/{dir}/{sample}.vcf'
    output:
        'output/{dir}/{sample}.tsv'
    shell:
        'gatk VariantsToTable '
        '-R {input.assembly} '
        '-V {input.vcf} '
        '-O {output} '
        '-F CHROM -F POS -GF GT -F QUAL -F MQ -F DP -F TRANSTION -GF AD '
        '-GF DP -F TYPE'

#
# rule calc_mutation_rate:
#     input:
#         'output/remove_filtered_variants/{{sample}}.{vartype}.vcf'
#     output:
#         'output/calc_mutation_rate/{sample}.{vartype}.tsv
#     shell:


# ==============================================================================
# Summarize Pipeline
# ==============================================================================

rule raw_fastqc:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        'output/raw_fastqc/{sample}.R1_fastqc.zip',
        'output/raw_fastqc/{sample}.R2_fastqc.zip',
    params:
        outdir = 'output/raw_fastqc'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads}"

rule clean_fastqc:
    input:
        fwd_reads = 'output/quality_trim/{sample}.R1.fastq.gz',
        rev_reads = 'output/quality_trim/{sample}.R2.fastq.gz',
        unpaired_reads = 'output/quality_trim/{sample}.unpaired.fastq.gz'
    output:
        'output/clean_fastqc/{sample}.R1_fastqc.zip',
        'output/clean_fastqc/{sample}.R2_fastqc.zip',
        'output/clean_fastqc/{sample}.unpaired_fastqc.zip'
    params:
        outdir = 'output/clean_fastqc'
    shell:
        'fastqc '
        '-o {params.outdir} '
        '{input.fwd_reads} {input.rev_reads} {input.unpaired_reads}'

rule quast:
    input:
        unpack(match_reads), unpack(match_assembly)
    output:
        'output/quast{iter}/{genotype}/report.tsv'
    threads: 6
    params:
        outdir = "output/quast{iter}/{genotype}"
    resources:
        mem_mb = 40000
    shell:
        'quast.py '
        '-o {params.outdir} '
        '--eukaryote '
        '-1 {input.fwd_reads} -2 {input.rev_reads} '
        '-t {threads} '
        '--no-snps {input.assembly}'

rule qualimap:
    input:
        'output/mark_duplicates/{sample}.bam'
    output:
        'output/qualimap/{sample}/genome_results.txt'
    threads: 8
    shell:
        'qualimap bamqc '
        '-nt {threads} '
        '-bam {input} '
        '-outdir output/qualimap/{wildcards.sample}'

rule busco:
    input:
        unpack(match_assembly)
    output:
        'output/busco{iter}/{genotype}/short_summary_{genotype}.txt'
    params:
        outdir = 'output/busco{iter}/{genotype}'
    run:
        outdir = local.path(params.outdir)
        clear_directory(outdir)
        outdir_root = outdir.dirname
        local.env['BUSCO_CONFIG_FILE'] = local.path(config['busco_config'])
        local.env['AUGUSTUS_CONFIG_PATH'] = local.path(
            config['augustus_config'])
        local.env['PATH'] = ':'.join([
            local.path(config['augustus_scripts']), local.env['PATH']
        ])
        assembly = local.path(input.assembly)
        lineage_file = local.path(config['busco_database'])
        tmp_outdir = outdir_root / ('run_' + wildcards.genotype)

        with local.cwd(outdir_root):
            busco['-i', assembly, '-o', wildcards.genotype, '-l', lineage_file,
                  '-m', 'geno', '-f'] & FG
            for f in tmp_outdir.list():
                f.move(outdir)
            tmp_outdir.delete()

rule samtools_stats:
    input:
        'output/align_to_assembly_final/{sample}.bam'
    output:
        'output/samtools_stats/{sample}.stats'
    shell:
        'samtools stats {input} > {output}'

rule multiqc:
    input:
        expand('output/{stage}_fastqc/{sample}.{dir}_fastqc.zip', sample=SAMPLE,
                dir=['R1', 'R2'], stage=['raw', 'clean']),
        expand('output/clean_fastqc/{sample}.unpaired_fastqc.zip',
               sample=SAMPLE),
        expand('output/merge_reads/{sample}.hist', sample=SAMPLE),
        expand('output/quast{iter}/{genotype}/report.tsv',
               iter=PLOT_ITER, genotype=GENOTYPE),
        expand('output/busco{iter}/{genotype}/short_summary_{genotype}.txt',
               iter=PLOT_ITER, genotype=GENOTYPE),
        expand('output/mark_duplicates/log/{sample}.log', sample=SAMPLE),
        expand('output/samtools_stats/{sample}.stats', sample=SAMPLE),
        expand('output/qualimap/{sample}/genome_results.txt', sample=SAMPLE)
    output:
        'output/multiqc/multiqc.html'
    shell:
        'multiqc -f -d -n {output} {input}'
