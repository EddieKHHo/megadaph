"""Megadaph Decontamination Pipeline"""
import os
from shutil import (
    move,
    rmtree,
)
from tempfile import NamedTemporaryFile
from uuid import uuid4

from plumbum import local
from plumbum.cmd import cat

# ==============================================================================
# Set up
# ==============================================================================
# {{{{
configfile: "config.yml"

SAMPLE, PAIR = glob_wildcards(
                              os.path.join(config['readsdir'], '{sample}.{pair}.fastq.gz'))

# Remove duplicate entries
SAMPLE = sorted(list(set(SAMPLE)))

# Genotype indices given by first two letters of sample name
GENOTYPE = list(set([x[0:2] for x in SAMPLE]))

GENOTYPE_SAMPLE = {}
for genotype in GENOTYPE:
    GENOTYPE_SAMPLE[genotype] = [x for x in SAMPLE if x.startswith(genotype)]

ITERATIONS = [1, 2]
FINAL_ITERATION = max(ITERATIONS)
# }}}}

# ==============================================================================
# Helper functions
# ==============================================================================
# {{{

def get_iterations_reads(wildcards):
    """Get the current set of reads which have passed filtering"""
    try:
        smp = wildcards.sample
    except AttributeError:
        smp = wildcards.genotype + 'SC'

    outdict = {}
    if int(wildcards.iter) == 1:
        outdict['fwd_reads'] = os.path.join('output', 'quality_trim', 
            smp + '.R1.fastq.gz'),
        outdict['rev_reads'] = os.path.join('output', 'quality_trim', 
            smp + '.R2.fastq.gz'),
        outdict['unpaired_reads'] = os.path.join('output', 'quality_trim', 
            smp + '.merged.fastq.gz'),
    else:
        prev_iter = str(int(wildcards.iter) - 1)
        outdict['fwd_reads'] = os.path.join(
            'output', 'filter_contamination'+ prev_iter,
            smp + '.R1.fastq.gz'),
        outdict['rev_reads'] = os.path.join(
            'output', 'filter_contamination'+ prev_iter,
            smp + '.R2.fastq.gz'),
        outdict['unpaired_reads'] = os.path.join(
            'output', 'filter_contamination'+ prev_iter,
            smp + '.unpaired.fastq.gz'),
    return outdict
    

def get_assembly_bams_from_genotype(wildcards):
    sample_names = get_samples_for_genotype(wildcards)
    bam_files = [''.join(["output/align_to_assembly", wildcards.iter, "/",
                         x, ".bam"])
                         for x in sample_names]
    return bam_files


def get_assembly_covfiles_from_genotype(wildcards):
    sample_names = get_samples_for_genotype(wildcards)
    cov_files = [''.join(["output/convert_alignment_to_cov", wildcards.iter,
                         "/", x, ".bam.cov"])
                         for x in sample_names]
    return cov_files


def get_samples_for_genotype(wildcards):
    return [x for x in SAMPLE if x.startswith(wildcards.genotype)]


def get_genotype_for_sample(wildcards):
    return ''.join(wildcards.sample[0:2])


def get_starting_control(wildcards):
    """Given a sample ID, return the ID of its starting control"""
    return wildcards.sample[0:2] + "SC"


def match_assembly(wildcards):
    """Given the sample, return the starting control assembly"""
    starting_control = get_starting_control(wildcards)
    assembly = os.path.join("output", 
                            "length_filter_assembly" + wildcards.iter,
                            starting_control + ".fasta")
    return assembly


def match_assembly_index(wildcards):
    """Given the sample, return the starting control assembly bowtie2 index"""
    starting_control = get_starting_control(wildcards)
    return os.path.join("output", 
                        "length_filter_assembly" + wildcards.iter,
                        starting_control + ".1.bt2")


def bowtie2_idx_root(idx):
    """Given the path to a forward bowtie2 index, return its root name"""
    return os.path.splitext(os.path.splitext(idx)[0])[0]

# }}}

# ==============================================================================
# Targets
# ==============================================================================
# {{{
rule all:
    input:
        'output/multiqc/multiqc.html',
        expand("output/produce_blobplots{iter}/{genotype}/family/{genotype}.bestsum"
               ".family.p20.span.100.blobplot.cov0.png", genotype=GENOTYPE,
               iter=ITERATIONS),
        expand("output/produce_covplots{iter}/{sample}.covsum.png",
               sample=SAMPLE, iter=ITERATIONS),
        expand("output/produce_covplot_excluding_bacteria/{sample}.covsum.png",
               sample=SAMPLE),
        expand("output/filter_contamination1/{sample}.R1.fastq.gz",
               sample=SAMPLE),
        expand("output/produce_blobtable{iter}/{genotype}.blobDB.bestsum.table.txt",
               genotype=GENOTYPE, iter=ITERATIONS)
# }}}

# ==============================================================================
# Build extra input files
# ==============================================================================
# {{{
rule build_diamond_database:
    output:
        "output/build_diamond_database/uniprot_ref_proteomes.fasta",
        "output/build_diamond_database/uniprot_ref_proteomes.dmnd",
        "output/build_diamond_database/uniprot_ref_proteomes.taxids"
    threads: 16
    script:
        "scripts/build_diamond_database.py"
# }}}

# ==============================================================================
# Quality Control / Trimming
# ==============================================================================
# {{{

rule trim_adapters:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        fwd_reads = 'output/trim_adapters/{sample}.R1.fastq.gz',
        rev_reads = 'output/trim_adapters/{sample}.R2.fastq.gz'
    log: "output/trim_adapters/log/{sample}.log"
    threads: 10
    params:
        ref = config['adapters'],
        k = '23',
        ktrim = 'r',
        mink = '4',
        hdist = '1'
    shell:
        "bbduk.sh in={input.fwd_reads} in2={input.rev_reads} threads=1 "
        "out={output.fwd_reads} out2={output.rev_reads} ref={params.ref} "
        "k={params.k} ktrim={params.ktrim} mink={params.mink} "
        "hdist={params.hdist} threads={threads} tpe tbo 2> {log}"


rule merge_reads:
    input:
        fwd_reads = rules.trim_adapters.output.fwd_reads,
        rev_reads = rules.trim_adapters.output.rev_reads
    output:
        fwd_reads = 'output/merge_reads/{sample}.R1.fastq.gz',
        rev_reads = 'output/merge_reads/{sample}.R2.fastq.gz',
        merged_reads = 'output/merge_reads/{sample}.merged.fastq.gz',
        insert_hist = 'output/merge_reads/{sample}.hist'
    log: "output/merge_reads/log/{sample}.log"
    params:
        vstrict = 't'
    shell:
        "bbmerge.sh in1={input.fwd_reads} in2={input.rev_reads} threads=1 "
        "out={output.merged_reads} outu={output.fwd_reads} "
        "outu2={output.rev_reads} ihist={output.insert_hist} "
        "vstrict={params.vstrict} 2> {log}"


rule quality_trim:
    input:
        fwd_reads = rules.merge_reads.output.fwd_reads,
        rev_reads = rules.merge_reads.output.rev_reads,
        merged_reads = rules.merge_reads.output.merged_reads
    output:
        fwd_reads = 'output/quality_trim/{sample}.R1.fastq.gz',
        rev_reads = 'output/quality_trim/{sample}.R2.fastq.gz',
        merged_reads = 'output/quality_trim/{sample}.merged.fastq.gz'
    log:
        paired = "output/quality_trim/log/{sample}.paired.log",
        unpaired = "output/quality_trim/log/{sample}.unpaired.log"
    threads: 10
    params:
        qtrim = 'rl',
        trimq = '20',
        minlen = '50'
    shell:
        "bbduk.sh in={input.fwd_reads} in2={input.rev_reads} "
        "out={output.fwd_reads} out2={output.rev_reads} threads={threads} "
        "qtrim={params.qtrim} trimq={params.trimq} minlen={params.minlen} "
        "2> {log.paired} &&"
        "bbduk.sh in={input.merged_reads} out={output.merged_reads} "
        "qtrim={params.qtrim} trimq={params.trimq} minlen={params.minlen} "
        "threads={threads} 2> {log.unpaired}"
# }}}

# ==============================================================================
# Assembly-Decontamination Iterations
# ==============================================================================
# {{{

rule assembly:
    input:
        unpack(get_iterations_reads)
    output:
        assembly = 'output/assembly{iter}/{genotype}SC.fasta',
        output_dir = 'output/assembly{iter}/{genotype}SC_spades'
    threads: 8
    resources:
        mem_mb = 50000
    shell:
        "python2 util/spades/bin/spades.py -1 {input.fwd_reads} -2 "
        "{input.rev_reads} -s {input.unpaired_reads} --threads {threads} "
        "-o {output.output_dir} && "
        "mv {output.output_dir}/scaffolds.fasta {output.assembly}"

rule length_filter_assembly:
    input:
        'output/assembly{iter}/{genotype}SC.fasta'
    output:
        'output/length_filter_assembly{iter}/{genotype}SC.fasta'
    params:
        minlen = "200"
    shell:
        "length_filter_fasta.pl {params.minlen} {input} > {output}"

rule bowtie2_index_assembly:
    input:
        'output/length_filter_assembly{iter}/{genotype}SC.fasta'
    output:
        'output/length_filter_assembly{iter}/{genotype}SC.1.bt2'
    params:
        output_prefix = "output/length_filter_assembly{iter}/{genotype}SC"
    shell:
        "bowtie2-build {input} {params.output_prefix}"

rule blast_assembly:
    input:
        'output/length_filter_assembly{iter}/{genotype}SC.fasta'
    output:
        "output/blast_assembly{iter}/{genotype}SC.tsv"
    threads: 10
    params:
        max_target_seqs = "5",
        evalue = "1e-25",
        max_hsps = "1",
    shell:
        "blastn -db {config[blast_database]} -task megablast -max_target_seqs "
        "{params.max_target_seqs} -max_hsps 1 -evalue {params.evalue} -query "
        "{input} -out {output} -outfmt \"6 qseqid staxids bitscore std\" "
        "-num_threads {threads}"


rule diamond_blast_assembly:
    input:
        assembly = 'output/length_filter_assembly{iter}/{genotype}SC.fasta',
        db = "output/build_diamond_database/uniprot_ref_proteomes.dmnd",
    output:
        "output/diamond_blast_assembly{iter}/{genotype}SC.out"
    params:
        max_target_seqs = "1",
        evalue = "1e-25"
    threads: 16
    resources:
        mem_mb = 50000
    shell:
        "diamond blastx --query {input.assembly} --max-target-seqs "
        "{params.max_target_seqs} --threads {threads} --sensitive "
        "--db {input.db} --evalue {params.evalue} --outfmt 6 --out {output}"

rule taxify_diamond_blasts:
    input:
        diamond_hits = "output/diamond_blast_assembly{iter}/{genotype}SC.out",
        taxids = "output/build_diamond_database/uniprot_ref_proteomes.taxids"
    output:
        "output/diamond_blast_assembly{iter}/{genotype}SC.taxified.out",
    params:
        output_prefix = "output/diamond_blast_assembly{iter}/"
    shell:
        "blobtools taxify -f {input.diamond_hits} -m {input.taxids} -s 0 -t 2 "
        "-o {params.output_prefix}"

rule align_to_assembly:
    input:
        unpack(get_iterations_reads),
        assembly = match_assembly,
        index = match_assembly_index
    output:
        bam = 'output/align_to_assembly{iter}/{sample}.bam',
        bai = 'output/align_to_assembly{iter}/{sample}.bam.bai'
    threads: 12
    resources:
        mem_mb = 20000
    log:
        "output/align_to_assembly{iter}/log/{sample}.log"
    run:
        index_prefix = bowtie2_idx_root(input.index)
        shell("align_and_sort.py -1 {input.fwd_reads} -2 {input.rev_reads} -U "
              "{input.unpaired_reads} -x " + index_prefix + " -p {threads} > "
              "{output.bam} 2> {log}")
        shell("samtools index {output.bam}")

rule convert_alignment_to_cov:
    input:
        bam = "output/align_to_assembly{iter}/{sample}.bam",
        assembly = lambda wildcards: ''.join([
            "output/length_filter_assembly", wildcards.iter, '/', 
            get_genotype_for_sample(wildcards), 'SC.fasta'])
    output:
        "output/convert_alignment_to_cov{iter}/{sample}.bam.cov"
    resources:
        mem_mb = 20000
    params:
        output_prefix = "output/convert_alignment_to_cov{iter}/"
    log:
        "output/convert_alignment_to_cov{iter}/logs/{sample}.log"
    shell:
        "blobtools map2cov -i {input.assembly} -b {input.bam} "
        "-o {params.output_prefix} 2> {log}"

rule init_blobtools_database:
    input:
        assembly = "output/length_filter_assembly{iter}/{genotype}SC.fasta",
        blast_hits = "output/blast_assembly{iter}/{genotype}SC.tsv",
        diamond_hits = "output/diamond_blast_assembly{iter}/{genotype}SC.taxified.out",
        covs = get_assembly_covfiles_from_genotype
    output:
        "output/init_blobtools_database{iter}/{genotype}.blobDB.json"
    resources:
        mem_mb = 20000
    params:
        output_prefix = "output/init_blobtools_database{iter}/{genotype}"
    log:
        "output/init_blobtools_database{iter}/log/{genotype}.log"
    run:
        cov_args = ' -c '.join(input.covs) 
        shell("blobtools create -i {input.assembly} "
              "--title {wildcards.genotype} --out {params.output_prefix} "
              "--hitsfile {input.blast_hits} --hitsfile {input.diamond_hits} "     
              "--nodes {config[nodes_dmp]} --names {config[names_dmp]} "
              "-c " + cov_args + " > {log} 2>&1")

rule produce_blobplots:
    input:
        "output/init_blobtools_database{iter}/{genotype}.blobDB.json"
    output:
        "output/produce_blobplots{iter}/{genotype}/family/{genotype}.bestsum.family.p20.span.100.blobplot.cov0.png",
        "output/produce_blobplots{iter}/{genotype}/genus/{genotype}.bestsum.genus.p20.span.100.blobplot.cov0.png",
        "output/produce_blobplots{iter}/{genotype}/order/{genotype}.bestsum.order.p20.span.100.blobplot.cov0.png",
        "output/produce_blobplots{iter}/{genotype}/phylum/{genotype}.bestsum.phylum.p20.span.100.blobplot.cov0.png",
        "output/produce_blobplots{iter}/{genotype}/species/{genotype}.bestsum.species.p20.span.100.blobplot.cov0.png"
    threads: 5
    resources:
        mem_mb = 40000
    params:
        output_prefix = "output/produce_blobplots{iter}/{genotype}"
    shell:
        "scripts/produce_blobplots.py -o {params.output_prefix} -d {input}"

rule produce_covplots:
    input:
        cov = "output/convert_alignment_to_cov{iter}/{sample}.bam.cov",
        db = lambda wildcards: ''.join(["output/init_blobtools_database",
                                        wildcards.iter, "/", 
                                        get_genotype_for_sample(wildcards),
                                        ".blobDB.json"])
    output:
        "output/produce_covplots{iter}/{sample}.covsum.png"
    params:
        output_prefix = "output/produce_covplots{iter}/{sample}",
        rank = "superkingdom",
        exclude = ''
    script:
        "scripts/produce_covplots.py"

rule produce_covplots_excluding_bacteria:
    input:
        cov = "output/convert_alignment_to_cov1/{sample}.bam.cov",
        db = lambda wildcards: ("output/init_blobtools_database1/" + 
            get_genotype_for_sample(wildcards) + ".blobDB.json")
    output:
        "output/produce_covplot_excluding_bacteria/{sample}.covsum.png"
    params:
        output_prefix = "output/produce_covplot_excluding_bacteria/{sample}",
        rank = "superkingdom",
        exclude = "Bacteria"
    script:
        "scripts/produce_covplots.py"

rule produce_blobtable:
    input:
        db = "output/init_blobtools_database{iter}/{genotype}.blobDB.json"
    output:
        "output/produce_blobtable{iter}/{genotype}.blobDB.bestsum.table.txt"
    shell:
        "blobtools view -r all -b -i {input.db} "
        "-o output/produce_blobtable{wildcards.iter}/"


# List the contigs which failed the filtering step
rule find_contam_contigs:
    input:
        "output/produce_blobtable1/{genotype}.blobDB.bestsum.table.txt"
    output:
        "output/find_contam_contigs1/{genotype}.txt"
    shell:
        "scripts/find_contam_contigs1.R {input} > {output}"

# List the contigs which passed the filtering step
rule generate_inclusion_list:
    input:
        failed_list = "output/find_contam_contigs1/{genotype}.txt",
        assembly = "output/assembly1/{genotype}SC.fasta"
    output:
        "output/generate_inclusion_list1/{genotype}.txt"
    shell:
        "list_csomes {input.assembly} | complement.sh /dev/stdin "
        "{input.failed_list} > {output}"


rule filter_contamination:
    input:
        #passing_contigs = lambda wildcards: ''.join([
        #    "output/generate_inclusion_list", wildcards.iter, "/", 
        #    get_genotype_for_sample(wildcards), ".txt"]),
        passing_contigs = lambda wildcards: ''.join([
            "output/generate_inclusion_list1/", 
            get_genotype_for_sample(wildcards), ".txt"]),
        bam = "output/align_to_assembly1/{sample}.bam"
    output:
        fwd = "output/filter_contamination1/{sample}.R1.fastq.gz",
        rev = "output/filter_contamination1/{sample}.R2.fastq.gz",
        unpaired = "output/filter_contamination1/{sample}.unpaired.fastq.gz"
    threads: 16
    resources:
        mem_mb = 40000
    params:
        output_prefix = "output/filter_contamination1/{sample}"
    log:
        "output/filter_contamination1/logs/{sample}.log"
    run:
        shell("extract_csomes.py -n 200 -p {threads} -o {params.output_prefix} "
        "-i {input.passing_contigs} -f fastq {input.bam} 2> {log}")
        outdir = local.path(output.unpaired).dirname
        tmp = outdir / (uuid4().hex + '.fastq.gz')
        # Remove all unpaired reads which are not merged
        shell("zcat {output.unpaired} | paste - - - - | awk "
              "'length($2) >= 151' | sed 's/\\t/\\n/g' | gzip - > " + str(tmp))
        move(str(tmp), output.unpaired)


# }}} 

# ==============================================================================
# Summarize Pipeline
# ==============================================================================
# {{{
        
rule raw_fastqc:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        'output/raw_fastqc/{sample}.R1_fastqc.html',
        'output/raw_fastqc/{sample}.R2_fastqc.html',
    params:
        outdir = 'output/raw_fastqc'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads}"

rule clean_fastqc:
    input:
        fwd_reads = 'output/quality_trim/{sample}.R1.fastq.gz',
        rev_reads = 'output/quality_trim/{sample}.R2.fastq.gz',
        unpaired_reads = 'output/quality_trim/{sample}.merged.fastq.gz'
    output:
        'output/clean_fastqc/{sample}.R1_fastqc.html',
        'output/clean_fastqc/{sample}.R2_fastqc.html',
        'output/clean_fastqc/{sample}.merged_fastqc.html'
    params:
        outdir='output/clean_fastqc'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads} "
        "{input.merged_reads}"

rule contam_filter_fastqc:
    input:
        fwd_reads = 'output/filter_contamination1/{sample}.R1.fastq.gz',
        rev_reads = 'output/filter_contamination1/{sample}.R2.fastq.gz',
        unpaired_reads = 'output/filter_contamination1/{sample}.unpaired' +
            '.fastq.gz'
    output:
        'output/contam_filter_fastqc1/{sample}.R1_fastqc.html',
        'output/contam_filter_fastqc1/{sample}.R2_fastqc.html',
        'output/contam_filter_fastqc1/{sample}.unpaired_fastqc.html'
    params:
        outdir = 'output/contam_filter_fastqc1'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads} "
        "{input.unpaired_reads}"
    
rule multiqc:
    input:
        expand('output/contam_filter_fastqc1/{sample}.R1_fastqc.html', 
               iter=ITERATIONS, sample=SAMPLE),
        expand('output/contam_filter_fastqc1/{sample}.R2_fastqc.html', 
               iter=ITERATIONS, sample=SAMPLE),
        expand('output/contam_filter_fastqc1/{sample}.unpaired_fastqc.html', 
               iter=ITERATIONS, sample=SAMPLE),
    output: 'output/multiqc/multiqc.html'
    shell:
        'multiqc -d -n {output} .'
# }}}
