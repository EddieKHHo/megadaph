Fenner Macrae Megadaph - Nuclear Analysis Lab Notebook
===============================================================================

Logs of specific commands run can be found in analysis directories.

===============================================================================
August 21st, 2017
===============================================================================

-------------------------------------------------------------------------------
Summary of project so far:
-------------------------------------------------------------------------------
- Emily has trimmed and merged reads (~/megadaph/reads/cleaned_reads/)

- Emily has produced draft assemblies for all starting controls using SPAdes
  (~/megadaph/assembly/spades_Aug2017/)

- I constructed a Kraken (https://ccb.jhu.edu/software/kraken/) database
containing all complete prokaryotic refseq assemblies
(~/megadaph/decontamination/kraken/db/). Kraken's database build scripts are
out of date, so I used a user produced set of scripts
(https://github.com/mw55309/Kraken_db_install_scripts). Protocol was followed
from (http://www.opiniomics.org/building-a-kraken-database-with-new-ftp-\
structure-and-no-gi-numbers/)

- I also downloaded the prebuilt full nt database for Centrifuge
  (https://ccb.jhu.edu/software/centrifuge)
===============================================================================
November 19th, 2017
===============================================================================

-------------------------------------------------------------------------------
Activity Log for today:
-------------------------------------------------------------------------------
- Ran centrifuge on cleaned FA_SC reads
  (~/megadaph/decontamination/centrifuge/reads) and the D. magna reference
assembly (~/megadaph/decontamination/centrifuge/reference_assembly). Used the
complete nt centrifuge database but excluded arthropods and bony fish. The
taxonomic IDs for these groups were found by examining
~/megadaph/decontamination/centrifuge/custom_db/taxonomy/names.dmp.


===============================================================================
August 22nd, 2017 - August 23rd, 2017
===============================================================================
- Constructed a custom centrifuge database using protocol from centrifuge
manual. All Bacterial, Plant, Protozoan, Archael and Fungal genomes classified
as 'Complete' by NCBI were included, as well as the human reference.

===============================================================================
August 23rd, 2017
===============================================================================
- Created an R package fen.R.util (https://github.com/fennerm/fen.R.util) to
house code shared between other R packages. Decided to make it separate to the
bioinformatics_scripts repo since it will primarily be updated from my personal
machine.

-------------------------------------------------------------------------------
Preliminary Centrifuge Analysis Of D. magna reference assembly
-------------------------------------------------------------------------------
(~/megadaph/decontamination/centrifuge/reference_assembly)
- Wrote R functions for adding taxonomic name, and match fraction to the
centrifuge output file.

Taxa with strong evidence for contamination:
- Thermus thermophilus (From Taq polymerase?)
- Homo sapiens
- Limnohabitans
- Plant of some kind (hits to tomato, rice, cucumber etc.)
- Parasitic worm (Trichobilharzia, Spirometra, Echinostoma, Protopolystoma,
Schistosoma)

===============================================================================
August 24th, 2017
===============================================================================
-------------------------------------------------------------------------------
Decontamination Strategy (to be continuously updated)
-------------------------------------------------------------------------------
- Identify confident contaminants with centrifuge and remove
- Reassemble and run centrifuge on assembly
- Use GC content and coverage to remove additional suspicious contigs
(See Koutsovoulos et al., 2016)
- Exclude contigs below 500bp
(See Koutsovoulos et al., 2016)
- Figure out plan for checking that assembly isn't overcleaned.
- Check for differences in relative coverage between different starting control
assemblies!
- Check for dramatic shifts in coverage across potential misassemblies

===============================================================================
August 25th, 2017
===============================================================================
Planned analysis:
- Goal: Produce confidence metric for centrifuge classifications.
- Strategy: Scramble all of the nucleotides in the fasta input to centrifuge.
Then run centrifuge. This will give us our expectations under the null.
- Wrote a script to scramble all of the sequences in a multifasta file
(fmacrae/code/R/scripts/scramble_fasta.R)
- Started test run of analysis on FA_SC in
megadaph/decontamination/centrifuge/null_distribution
- Found a bunch of false positive hits. A bunch to human, sheep, tomato etc.
Basically all large genomes. Largest hit score was only 289 though, so
filtering by score should be pretty easy and effective.

===============================================================================
August 28th, 2017
===============================================================================
- Large reorganization of project github. Local and IBEST directories should
now be kept fairly well in sync using github. Data will still reside only on
IBEST.
- Wrote the alignment function for the megadaph pipeline
(fmacrae/code/util/python_modules/run_bowtie2.py)

===============================================================================
August 29th, 2017
===============================================================================
- Added run_index_fasta module to the Megadaph decontamination pipeline.
- Added CheckedArgs class module to validate arguments passed to the pipeline.
- Added unit tests directory, and got testing working for CheckedArgs

===============================================================================
August 30th, 2017
===============================================================================
- Setup my personal python modules as a package (fmbiopy) for easier import and
extensibility.
	- Using pytest for package testing (https://docs.pytest.org/en/latest/)

- Plan to set up fmbiopy and bioinformatics_scripts as subtrees of the Megadaph
repository, but the version of git on IBEST is super out of date. Messaged
Benji about getting a new version. Will use
https://medium.com/@porteneuve/mastering-git-subtrees-943d29a798ec as a guide
for subtree set up.

===============================================================================
August 31st, 2017
===============================================================================
- Fixed bugs in run_bowtie2.py, run_index_fasta.py and fen_util.run_command().
All tests now passing
- Came up with an object oriented pipeline structure. Basically will define
pipeline and submodule classes which will automatically handle argument
passing, logging and argument checking.
- Benji made an updated git module but its only available for some of the
servers until they update the rest. Don't know how long this will take but I'll
put off getting the subtree properly set up for the time being
- Read about pytest features:
http://pythontesting.net/framework/pytest/pytest-fixtures-nuts-bolts/#bare

===============================================================================
September 3rd, 2017
===============================================================================
- Partially created BioFileGroup class for housing and type checking
bioinformatics files within the pipeline (~/fmacrae/code/fmbiopy/fmbiopy.py).
Some tests still failing. Tomorrow will start by refactoring the prevalidation
functions.

===============================================================================
September 4th, 2017
===============================================================================
- Added a bunch of classes to BioFileGroup.

===============================================================================
September 5th, 2017
===============================================================================
- Added further classes to BioFileGroup. Now just need to implement an
IndexedFasta class and I can move onto the pipeline code.
- IndexedFasta implemented. Documentation could still use a little work but
biofile.py is basically complete.
- Installed pydocstring plugin for easy documentation in reST formatting. Call
with :Pydocstring in vim while cursor is on function or class definition.
- Thought a lot about what docstring style to use from now on. Decided on numpy
style for the following reasons:
	- Ease of reading
	- Can use 'pyment -wo "numpydoc" <file.py>' to automate much of the process
	- Can be converted into documentation with Sphinx
	- Guidelines:
	  http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html

===============================================================================
September 6th, 2017
===============================================================================
- Think I might have reinvented the wheel a bit with biofile. Ruffus seems like
it includes the same functionality and more.
- New plan: Next step rewrite bowtie2_align.py and index_fasta.py as ruffus
functions
- Interrupted while working: currently working on editing the .vimrc to auto-
open split tabs with python environment

===============================================================================
September 7th, 2017
===============================================================================
- Officially switched over to Ruffus for pipelining.
- Set up Ruffus logging class in fmbiopy.fmruffus

===============================================================================
September 10th, 2017
===============================================================================
- Wrote and tested following tasks in fmbiopy.ruffus_tasks: bowtie_index_fasta,
samtools_index_fasta, gunzip, gzip, paired_bowtie2_align

===============================================================================
September 11th, 2017
===============================================================================
- Spent a while trying to get jedi-vim to play nice with vim 8 and pyenv.
Eventually got it to work by adding a script to .vim/after/ftplugin/python.vim,
and adding an __init__.py to fmbiopy.fmbiopy
- Started on the decontam_pipeline proper in
  'fmacrae/megadaph/decontam_pipeline'. Decided to structure as a script rather
than a package.
- Started adding some additional functionality to biofile, so that it can be
used for checking argument validity in the pipeline.

===============================================================================
September 13th, 2017
===============================================================================
- Ideas from meeting
- Primers arrived
- Meeting with Sarah: Updated on progress. We shared a little concern about
Emily using hard GC content/coverage cutoffs for filtering contamination in the
assemblies. She showed me Emily's assembly stats though and they were super
impressive. Very large N50s and a pretty consistent total assembly length
~130-180 Mb.
- Lab meeting: Introductions - my presentation slot set for Nov. 29th
- Met with Emily about assembly progress. We talked about the hard cutoff
criteria issue. I suggested running the published reference genome through her
filter and see how much is lost. A nontrivial amount was excluded - large
enough that I would be uncomfortable using these filters for the mutation
calling pipeline. However we agreed they are probably fine for the TE detection
if she is comfortable with them. Given her timeframe, she probably doesn't have
the time to work at my snailpace. I asked her about her assembly parameters so
that I could eventually include them in the Ruffus pipeline.


===============================================================================
September 24th, 2017
===============================================================================
Been having troubles getting vim and pyenv to work with the new server
infrastructure which Benji is updating. Recompiled vim with following commands:

pyenv local 3.5.2

./configure --prefix=$HOME --with-features=huge --enable-multibyte
--enable-cscope=yes ---enable-pythoninterp=dynamic
--with-python3-config-dir=/opt/modules/devel/python/2.7.10/lib/python2.7/config
--enable-python3interp=dynamic
--with-python3-config-dir=/opt/modules/devel/python/3.5.2/lib/python3.5/config-3.5m/
--disable-gui

make VIMRUNTIMEDIR=$HOME/.vim

This worked to get vim working with python3 sort of, but broke python2. Also
syntax highlighting stopped working.

Decided to just switch to Neovim to avoid these issues. First installed
linuxbrew. Linuxbrew complained about our git version being out of date.
Attempted to update it using 'linuxbrew install git', but its throwing the
following error:

/mnt/lfs2/schaack/.linuxbrew/bin/ld: cannot find -ldb
collect2: error: ld returned 1 exit status

===============================================================================
September 25th, 2017
===============================================================================
Installed a brewed version of perl with linuxbrew.

===============================================================================
September 27th, 2017
===============================================================================
Installed brewed versions of git and neovim.

===============================================================================
September 28th, 2017
===============================================================================
Neovim up and running. Installed vim-plug for managing plugins. Added Ale for
syntax checking and vim-test for testing code in nvim.

===============================================================================
September 29th, 2017
===============================================================================
Refactored my dotfile set up so that my custom dotfiles are copied over ssh
upon login. This means that any changes to my dotfiles wont affect other users
and can also be more easily transferred to new servers.

Finally got neovim to run entirely without errors with my setup.

===============================================================================
October 2nd, 2017
===============================================================================
Spent all day bug fixing the decontamination pipeline

===============================================================================
October 4th, 2017
===============================================================================
Finally finished major bugfixing on fmruffus module. Architecture for running
Ruffus tasks is now pretty solid. Should now be fairly simple to add new
Ruffus tasks. Next step is to get a basic pipeline running with just the
initial Bowtie2 alignment to get a feel for how Ruffus pipeline scripts work.

===============================================================================
October 5th, 2017
===============================================================================
Minor bugfixes in fmruffus

===============================================================================
October 7th, 2017
===============================================================================
Bugfixing and refactoring in fmbiopy test functions

===============================================================================
October 8th, 2017
===============================================================================
Started on the actual pipeline script. Got the bowtie2 alignment step working
on small test files.

===============================================================================
October 9th, 2017
===============================================================================
Got a little stuck in the mud refactoring biofile. Decided that it makes more
sense to have the base class be a singular Biofile rather than a BiofileGroup.
Fixing this requires moving the bulk of the validation steps into the new
Biofile class and refactoring all of the non-generic BiofileGroup classes to
singular Biofile classes. The generic BiofileGroup will stick around as a way
to store a set of Biofiles, and do validation checks which pertain to groups.

===============================================================================
October 10th, 2017
===============================================================================
Worked on biofile refactoring

===============================================================================
October 11th, 2017
===============================================================================
Biofile refactoring mostly done. Generic Biofile class now works, as do Fasta
and Fastq. Commented out BiofileGroups and MatchedPrefixGroups for now. Will
fix them later if they end up being useful.

Created small Bam and Sam files with 10000 reads in testdat by subsampling
megadaph data with samtools.

===============================================================================
October 12th, 2017
===============================================================================
-Created small paired fastq files with 10000 reads by subsampling the megadaph
reads with seqtk for use in testdat.
-Did the same thing (1000 contigs) with the September 2017 redundans Megadaph
assemblies.
-Cloned the new testdat repository into fmbiopy and the megadaph
decontam_pipeline.
-Rewrote fmbiopy functions which rely on testdat

===============================================================================
October 14th, 2017
===============================================================================
- Switched from os.path to pathlib.Path for path manipulation in fmbiopy.
- Added BiofileGroup and MatchedPrefixGroup classes back to fmbiopy.biofile
- Worked on input file validation in megadaph pipeline

===============================================================================
October 15th, 2017
===============================================================================
- Got input validation working. Pipeline script now capable of producing a
  flowchart representing the pipeline.
- Added apply function to fmruffus which applies a single input RuffusTask to
  multiple inputs.

===============================================================================
October 16th, 2017
===============================================================================
- Got test functions to work outside of the test directory. Can now be run from
  the python package home folder.

===============================================================================
October 17th, 2017
===============================================================================
- Refactored, reorganized a bunch of fmbiopy code.
- Alignment and symlink parts of pipeline now working.
- Wrote RuffusTask for building a custom centrifuge index but didn't get time
  to test it

===============================================================================
October 18th, 2017
===============================================================================
- Bug fixed fmruffus.RuffusLog
- Added fixtures to fmtest

===============================================================================
October 19th, 2017
===============================================================================
- Added RuffusTransform class and altered RuffusTask class to handle
  ruffus.Pipeline.originate tasks.

===============================================================================
October 20th, 2017
===============================================================================
- Added centrifuge task to fmbiopy.fmruffus

===============================================================================
October 21st, 2017
===============================================================================
- Had to do a fresh linuxbrew install due to some persistent issues with
  glibc6.
- Wrote script for converting centrifuge output to blobtools compatible hits
  file ('fmscripts/python/centrifuge_to_hits.py')

===============================================================================
October 24th, 2017
===============================================================================
- Improved fmpaths test coverage

===============================================================================
October 25-30th, 2017
===============================================================================
- Major connection difficulties with IBEST
- Used the intermediate time to work on other project and improve my dotfiles.

===============================================================================
November 1st, 2017
===============================================================================
- Received assembly and read processing scripts from Emily.

===============================================================================
November 2nd, 2017
===============================================================================
- Started adding Emily's analyses to the pipeline. Everything should be fairly
  easy except her script for removing heterozygosity. It explicitly repeats
  the same protocol for each file rather than looping so it'll need to be
  completely rewritten.

===============================================================================
November 4th-5th, 2017
===============================================================================
- Wrote Ruffus functions for all of Emily's tasks with the exception of the
  custom script mentioned above.

===============================================================================
November 6th, 2017
===============================================================================
- Moved fmbiopy.fmruffus and fmbiopy.biofile into their own github repos.

===============================================================================
November 7th, 2017
===============================================================================
- Split duffus into multiple module files.

Met with Emily about her strategy for reducing heterozygosity. Basically she
just removed all heterozygosity because it wasn't relevant to transposable
element discovery. I'll have to come up with a different solution.

Found an interesting looking tool called HaploMerger2 which rebuilds both
haploid assemblies from a heterozygous diploid assembly.

===============================================================================
November 8th and 10th, 2017
===============================================================================
- Worked on custom duffus Pipeline class.

===============================================================================
November 11th, 2017
===============================================================================
- The more time I spend looking at the Ruffus source code, the more I think its
  pretty poorly constructed. I've been spending an awful amount of time getting
  its Input-> Output formatter to work and I think its probably a waste of
  time. Going to cut my losses and switch to an alternative pipelining
  infrastructure. Snakemake seems like a good choice.

===============================================================================
November 14th, 2017
===============================================================================
- Made some good progress on the new snakemake pipeline. Got Emily's data set
  up in the pipeline folder so we don't have to rebuild all of it.

===============================================================================
November 15th, 2017
===============================================================================
- Started to test blobtools on a single file in
  'megadaph.private/data/exploratory/decontam/blobtools'. Hoping to nail down
  the blobtools part of the protocol here.

===============================================================================
November 16th, 2017
===============================================================================
- Decent portion of the exploratory blobtools pipeline finished. Currently
  running the initial alignment and centrifuge steps on trillian.

===============================================================================
November 17th, 2017
===============================================================================
- Job got stuck on Trillian for some reason. Restarted on Ford.

===============================================================================
November 18th, 2017
===============================================================================
- Exploratory pipeline has produced most of the inputs to blobtools and is now
  working on creating the blobtools database.
- Added the trimming steps to the main snakemake pipeline but I'm running into
  persistant syntax errors at the start of the script.

===============================================================================
November 19th, 2017
===============================================================================
- Blobtools is dependent on python2 but calls its submodules using 'python' so
  it ends up defaulting to python3 and failing. I fixed this by replacing all
  instances of python with python2 in the main script but a more intelligent
  solution might be required for the main pipeline. Submitted an issue on
  github.

- Can't get the initial symlink step to work in the main snakemake pipeline.
  Snakemake seems to have some trouble with symlinks in general so I'm just
  going to remove the step for now.

- Got the snakemake pipeline working all the way through to production of
  multiqc files. Will start running it on the full dataset.

- Got some initial blobplots from the blobplot pipeline.

===============================================================================
November 20th, 2017
===============================================================================
- Wrote a script for downloading a new custom centrifuge database and added it
  to the main snakemake pipeline.
- Started running the pipeline on the actual datafiles. When(if) this completes
  all fastq cleaning, QC and database downloading will be done.

===============================================================================
November 21st, 2017
===============================================================================
- Centrifuge kept encountering time out errors while connecting to NCBI. Online
  discussions suggested that this only occurs when centrifuge-download uses
  rsync. Edited the binary in ~/bin to use curl instead. Main pipeline is now
  running on Tesla.

- Meanwhile I've been testing SLURM scheduling of the pipeline to see if this
  leads to faster execution.

===============================================================================
November 22nd, 2017
===============================================================================
- Getting more errors with centrifuge-download. I'm going to split the
  centrifuge database building part of the pipeline into multiple tasks so that
  I don't have to start fresh each time it fails.

===============================================================================
November 23rd, 2017
===============================================================================
- Decided to rerun the exploratory blobtools pipeline with the entire nt
  database. It probably makes more sense to filter out unrealistic hits
  afterwards than spending too much time curating a custom database. Only thing
  that I'm worried about is that the centrifuge docs are very ambiguous as to
  the contents of their nt database. In the manual it says that the database
  contains all 'spliced coding' sequence. However in their paper etc. they say
  that its equivalent to the database used in BLAST.

===============================================================================
November 23rd, 2017
===============================================================================
- The main pipeline encountered errors in Fastqc building. This resulted from
  my removal of run_command in my fmbiopy library. Moved Fastqc from a script
  to a simple shell command, tested and resubmitted with SLURM.

- Very unsatisfied with the output I'm getting from Centrifuge. Going to try
  switching to BLAST for taxonomic assignment.


===============================================================================
November 27th, 2017
===============================================================================
- Got BLAST integrated into blobtools pipeline and running on slarti.

===============================================================================
November 28th, 2017
===============================================================================
- Fine tuned many aspects of exploratory pipeline - # BLAST hits per contig
  increased from 1 to 10, added a simple filter script to remove obvious
  contaminants. All done very messily to get done in time for presentation at
  lab meeting. Will need to go over again later.

- Tried to get Haplomerger2 set up. Its written very poorly so it'll take a lot
  of work to get it up and running.


-------------------------------------------------------------------------------
Pipeline for decontamination
-------------------------------------------------------------------------------
- Assemble
- Diploidify with haplomerger2
- Align to assembly
- blobtools pipeline
- Group by taxonomy and Concoct.
- Remove concoct groups which contain contaminants
- Remove obvious contaminants
- Possibly do a hard filter by coverage/GC if required
- Reassemble

===============================================================================
November 29th, 2017
===============================================================================
- Presented in lab meeting.

===============================================================================
November 30th, 2017
===============================================================================
- Started the initial SPADEs assemblies in the snakemake pipeline on SLURM.

===============================================================================
December 1st, 2017
===============================================================================
- Got haplomerger2 working on the provided test dataset. Had to install a bunch
  of ucsc-* packages from bioconda, and add an old version of libpng to my
  linker path.

===============================================================================
January 12th, 2018
===============================================================================
- The SPADEs assemblies finally completed. They a long time due to congestion
  on the server, but hopefully subsequent assemblies should be snappier.


