"""Megadaph Decontamination Pipeline"""

import os

configfile: "config.yaml"

SAMPLE, PAIR = glob_wildcards(
        os.path.join(config['readsdir'], '{sample}.{pair}.fastq.gz'))

if config['isolates'] != 'all':
    # Only include samples from the specified isolates
    if len(config['isolates']) == 1:
        isolates = [config['isolates']]
    else:
        isolates = config['isolates']
    SAMPLE = [x for x in SAMPLE if x.startswith(*isolates)]

STARTCONTROL = [x for x in SAMPLE if 'SC' in x]

rule all:
    input:
        'multiqc/multiqc.html'


rule raw_fastqc:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        'raw_fastqc/{sample}.R1_fastqc.html',
        'raw_fastqc/{sample}.R2_fastqc.html',
    params:
        outdir = 'raw_fastqc'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads}"


rule trim_adapters:
    input:
        fwd_reads = 'input/reads/{sample}.R1.fastq.gz',
        rev_reads = 'input/reads/{sample}.R2.fastq.gz'
    output:
        fwd_reads = 'trim_adapters/{sample}.R1.fastq.gz',
        rev_reads = 'trim_adapters/{sample}.R2.fastq.gz'
    log: "trim_adapters/log/{sample}.log"
    params:
        ref = config['adapters'],
        k = '23',
        ktrim = 'r',
        mink = '4',
        hdist = '1'
    shell:
        "bbduk.sh in={input.fwd_reads} in2={input.rev_reads} threads=1 "
        "out={output.fwd_reads} out2={output.rev_reads} ref={params.ref} "
        "k={params.k} ktrim={params.ktrim} mink={params.mink} "
        "hdist={params.hdist} tpe tbo 2> {log}"


# rule error_correction:
#   input:
#       fwd_reads = rules.trim_adapters.output.fwd_reads,
#       rev_reads = rules.trim_adapters.output.rev_reads
#   output:
#       fwd_reads = 'error_correction/{sample}.R1.fastq.gz',
#       rev_reads = 'error_correction/{sample}.R2.fastq.gz',
#   log: "error_correction/log/{sample}.log"
#   params:
#       k = '50'
#   shell:
#       """
#       tadpole.sh in={input.fwd_reads} in2={input.fwd_reads} \
#           out={output.fwd_reads} out2={output.rev_reads} k={params.k} \
#           mode=correct 2> {log}
#       """

rule merge_reads:
    input:
        fwd_reads = rules.trim_adapters.output.fwd_reads,
        rev_reads = rules.trim_adapters.output.rev_reads
    output:
        fwd_reads = 'merge_reads/{sample}.R1.fastq.gz',
        rev_reads = 'merge_reads/{sample}.R2.fastq.gz',
        merged_reads = 'merge_reads/{sample}.merged.fastq.gz',
        insert_hist = 'merge_reads/{sample}.hist'
    log: "merge_reads/log/{sample}.log"
    params:
        vstrict = 't'
    shell:
        "bbmerge.sh in1={input.fwd_reads} in2={input.rev_reads} threads=1 "
        "out={output.merged_reads} outu={output.fwd_reads} "
        "outu2={output.rev_reads} ihist={output.insert_hist} "
        "vstrict={params.vstrict} 2> {log}"


rule quality_trim:
    input:
        fwd_reads = rules.merge_reads.output.fwd_reads,
        rev_reads = rules.merge_reads.output.rev_reads,
        merged_reads = rules.merge_reads.output.merged_reads
    output:
        fwd_reads = 'quality_trim/{sample}.R1.fastq.gz',
        rev_reads = 'quality_trim/{sample}.R2.fastq.gz',
        merged_reads = 'quality_trim/{sample}.merged.fastq.gz'
    log:
        paired = "quality_trim/log/{sample}.paired.log",
        unpaired = "quality_trim/log/{sample}.unpaired.log"
    params:
        qtrim = 'rl',
        trimq = '20',
        minlen = '50'
    shell:
        "bbduk.sh in={input.fwd_reads} in2={input.rev_reads} "
        "out={output.fwd_reads} out2={output.rev_reads} threads=1 "
        "qtrim={params.qtrim} trimq={params.trimq} minlen={params.minlen} "
        "2> {log.paired} &&"
        "bbduk.sh in={input.merged_reads} out={output.merged_reads} threads=1 "
        "qtrim={params.qtrim} trimq={params.trimq} minlen={params.minlen} "
        "2> {log.unpaired}"


rule clean_fastqc:
    input:
        fwd_reads = rules.quality_trim.output.fwd_reads,
        rev_reads = rules.quality_trim.output.rev_reads,
        merged_reads = rules.quality_trim.output.merged_reads
    output:
        'clean_fastqc/{sample}.R1_fastqc.html',
        'clean_fastqc/{sample}.R2_fastqc.html',
        'clean_fastqc/{sample}.merged_fastqc.html'
    params:
        outdir='clean_fastqc'
    shell:
        "fastqc -o {params.outdir} {input.fwd_reads} {input.rev_reads} "
        "{input.merged_reads}"


rule multiqc:
    input:
        expand('clean_fastqc/{sample}.R1_fastqc.html', sample=SAMPLE),
        expand('clean_fastqc/{sample}.R2_fastqc.html', sample=SAMPLE),
        expand('clean_fastqc/{sample}.merged_fastqc.html', sample=SAMPLE),
        expand('raw_fastqc/{sample}.R1_fastqc.html', sample=SAMPLE),
        expand('raw_fastqc/{sample}.R2_fastqc.html', sample=SAMPLE)
    output: 'multiqc/multiqc.html'
    shell:
        'multiqc -d -n {output} .'


#rule download_taxonomy:
#    output:
#        'centdb/taxonomy/nodes.dmp', 'centdb/taxonomy/names.dmp'
#    shell:
#        "centrifuge-download -o centdb/taxonomy taxonomy"
#
#rule download_archaea:
#    output:
#        'centdb/archaea.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -P {threads} -d archaea "
#        "refseq > centdb/archaea.map"
#
#rule download_bacteria:
#    output:
#        'centdb/archaea.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -P {threads} -d bacteria "
#        "refseq > centdb/bacteria.map"
#
#
#rule download_fungi:
#    output:
#        'centdb/fungi.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d fungi "
#        "refseq > centdb/fungi.map"
#
#
#rule download_protozoa:
#    output:
#        'centdb/protozoa.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d protozoa "
#        "refseq > centdb/protozoa.map"
#
#
#rule download_viral:
#    output:
#        'centdb/viral.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d viral "
#        "refseq > centdb/viral.map"
#
#rule download_invertebrate:
#    output:
#        'centdb/invertebrate.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d invertebrate "
#        "refseq > centdb/invertebrate.map"
#
#rule download_human:
#    output:
#        'centdb/human.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d "
#        "vertebrate_mammalian -a Chromosome -t 9606 -c 'reference genome' "
#        "refseq > centdb/human.map"
#
#
#rule download_daphnia:
#    output:
#        'centdb/daphnia.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d "
#        "invertebrate -a Scaffold -t '6669,35525' refseq > centdb/dapnia.map"
#
#
#rule download_mitosporidium:
#    output:
#        'centdb/mitosporidium.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d "
#        "fungi -a contig -t 1485682 refseq > centdb/mitosporidium.map"
#
#rule download_algae:
#    output:
#        'centdb/algae.map'
#    threads: config['threadsmax']
#    shell:
#        "centrifuge-download -o centdb/library -m -p {threads} -d "
#        "plant -a Contig -t 3089 refseq > centdb/algae.map && "
#        "centrifuge-download -o centdb/library -m -p {threads} -d "
#        "plant -a Scaffold -t "
#        "'3055,3067,383055,145388,3075,1579116,53269,1470871,1621096,3078,"
#        "869301,1735750,3077,36881,33097,35704,51683,1157961,28458,1917417,"
#        "53269,1470871,1621096,3078,869301,3044,1157962,3088,70448,3075,"
#        "215448' refseq > centdb/algae.map"
#
#
#rule concatenate_tax_id_maps:
#    input:
#        'centdb/archaea.map',
#        'centdb/bacteria.map'
#        'centdb/fungi.map'
#        'centdb/protozoa.map'
#        'centdb/viral.map'
#        'centdb/invertebrate.map'
#
#rule build_centrifuge_db:
#   output:
#       db_prefix = 'centdb/megadaph-custom',
#       nodes = 'centdb/taxonomy/nodes.dmp',
#       names = 'centdb/taxonomy/names.dmp'
#   priority: 1
#   params:
#       outdir = 'centdb'
#   threads: config['threadsmax']
#   script: os.path.join(config['scriptdir'], 'centrifuge_download.py')

